{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\n",
    "from autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\") # Load environment variables from a .env file\n",
    "\n",
    "azure_ai_service_name = os.getenv(\"AZURE_AI_SERVICE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the token provider\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    model = \"gpt-4o\",\n",
    "    azure_endpoint=f\"https://{azure_ai_service_name}.openai.azure.com/\",\n",
    "    azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    # api_key=\"sk-...\", # For key-based authentication.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkerTask:\n",
    "    task: str\n",
    "    previous_results: List[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkerTaskResult:\n",
    "    result: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserTask:\n",
    "    task: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FinalResult:\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerAgent(RoutedAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_client: ChatCompletionClient,\n",
    "    ) -> None:\n",
    "        super().__init__(description=\"Worker Agent\")\n",
    "        self._model_client = model_client\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:\n",
    "        if message.previous_results:\n",
    "            # If previous results are provided, we need to synthesize them to create a single prompt.\n",
    "            system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n",
    "            system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(message.previous_results)])\n",
    "            model_result = await self._model_client.create(\n",
    "                [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n",
    "            )\n",
    "        else:\n",
    "            # If no previous results are provided, we can simply pass the user query to the model.\n",
    "            model_result = await self._model_client.create([UserMessage(content=message.task, source=\"user\")])\n",
    "        assert isinstance(model_result.content, str)\n",
    "        print(f\"{'-'*80}\\nWorker-{self.id}:\\n{model_result.content}\")\n",
    "        return WorkerTaskResult(result=model_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent(RoutedAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_client: ChatCompletionClient,\n",
    "        worker_agent_types: List[str],\n",
    "        num_layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__(description=\"Aggregator Agent\")\n",
    "        self._model_client = model_client\n",
    "        self._worker_agent_types = worker_agent_types\n",
    "        self._num_layers = num_layers\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:\n",
    "        print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived task: {message.task}\")\n",
    "        # Create task for the first layer.\n",
    "        worker_task = WorkerTask(task=message.task, previous_results=[])\n",
    "        # Iterate over layers.\n",
    "        print(f\"Number of layers: {self._num_layers}\")\n",
    "        for i in range(self._num_layers - 1):\n",
    "            # Assign workers for this layer.\n",
    "            worker_ids = [\n",
    "                AgentId(worker_type, f\"{self.id.key}/layer_{i}/worker_{j}\")\n",
    "                for j, worker_type in enumerate(self._worker_agent_types)\n",
    "            ]\n",
    "            # Dispatch tasks to workers.\n",
    "            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nDispatch to workers at layer {i}\")\n",
    "            results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])\n",
    "            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived results from workers at layer {i}\")\n",
    "            # Prepare task for the next layer.\n",
    "            worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])\n",
    "        # Perform final aggregation.\n",
    "        print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nPerforming final aggregation\")\n",
    "        system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n",
    "        system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(worker_task.previous_results)])\n",
    "        model_result = await self._model_client.create(\n",
    "            [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n",
    "        )\n",
    "        assert isinstance(model_result.content, str)\n",
    "        return FinalResult(result=model_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"I have 432 cookies, and divide them 3:4:2 between Alice, Bob, and Charlie. How many cookies does each person get?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "await WorkerAgent.register(\n",
    "    runtime, \"worker\", lambda: WorkerAgent(az_model_client)\n",
    ")\n",
    "await OrchestratorAgent.register(\n",
    "    runtime,\n",
    "    \"orchestrator\",\n",
    "    lambda: OrchestratorAgent(\n",
    "        model_client=az_model_client, worker_agent_types=[\"worker\"] * 3, num_layers=3\n",
    "    ),\n",
    ")\n",
    "\n",
    "runtime.start()\n",
    "result = await runtime.send_message(UserTask(task=task), AgentId(\"orchestrator\", \"default\"))\n",
    "await runtime.stop_when_idle()\n",
    "print(f\"{'-'*80}\\nFinal result:\\n{result.result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
